{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier for MNIST (Accuracy : 90 % )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 4.206151830824942\n",
      "Epoch: 0002 cost= 1.064519325332209\n",
      "Epoch: 0003 cost= 0.754769439751451\n",
      "Epoch: 0004 cost= 0.624664001844146\n",
      "Epoch: 0005 cost= 0.549987218461253\n",
      "Epoch: 0006 cost= 0.499466101120819\n",
      "Epoch: 0007 cost= 0.462549605654044\n",
      "Epoch: 0008 cost= 0.434041788862510\n",
      "Epoch: 0009 cost= 0.410897218205712\n",
      "Epoch: 0010 cost= 0.391832734766332\n",
      "Epoch: 0011 cost= 0.375845715566115\n",
      "Epoch: 0012 cost= 0.361874465292151\n",
      "Epoch: 0013 cost= 0.350139799673449\n",
      "Epoch: 0014 cost= 0.339499560228803\n",
      "Epoch: 0015 cost= 0.330736623961817\n",
      "Epoch: 0016 cost= 0.322719656906344\n",
      "Epoch: 0017 cost= 0.315982767682184\n",
      "Epoch: 0018 cost= 0.309034349146214\n",
      "Epoch: 0019 cost= 0.303616833944212\n",
      "Epoch: 0020 cost= 0.298947069665248\n",
      "Epoch: 0021 cost= 0.294551029232415\n",
      "Epoch: 0022 cost= 0.289806765670126\n",
      "Epoch: 0023 cost= 0.286411989080635\n",
      "Epoch: 0024 cost= 0.282308796766130\n",
      "Epoch: 0025 cost= 0.278503044444052\n",
      "Epoch: 0026 cost= 0.276322598755359\n",
      "Epoch: 0027 cost= 0.274482793211937\n",
      "Epoch: 0028 cost= 0.270718451291323\n",
      "Epoch: 0029 cost= 0.268310511626980\n",
      "Epoch: 0030 cost= 0.266122440573844\n",
      "Epoch: 0031 cost= 0.263750313208862\n",
      "Epoch: 0032 cost= 0.262354560778900\n",
      "Epoch: 0033 cost= 0.260129245363853\n",
      "Epoch: 0034 cost= 0.258378530903296\n",
      "Epoch: 0035 cost= 0.257443047233603\n",
      "Epoch: 0036 cost= 0.255090127059005\n",
      "Epoch: 0037 cost= 0.254130770523440\n",
      "Epoch: 0038 cost= 0.252950048351830\n",
      "Epoch: 0039 cost= 0.251092317036607\n",
      "Epoch: 0040 cost= 0.250227465995333\n",
      "Epoch: 0041 cost= 0.249060246632858\n",
      "Epoch: 0042 cost= 0.248082930879159\n",
      "Epoch: 0043 cost= 0.246845836083998\n",
      "Epoch: 0044 cost= 0.246415628337047\n",
      "Epoch: 0045 cost= 0.245479200631380\n",
      "Epoch: 0046 cost= 0.244520169916478\n",
      "Epoch: 0047 cost= 0.243176761337302\n",
      "Epoch: 0048 cost= 0.242263917746869\n",
      "Epoch: 0049 cost= 0.241966179920869\n",
      "Epoch: 0050 cost= 0.241206024241718\n",
      "Learning Finished!\n",
      "Accuracy: 0.9211\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32,[None,784])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "W= tf.Variable(tf.random_normal([784,10]))\n",
    "b=tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(X,W)+b\n",
    "\n",
    "\n",
    "learning_rate=0.001\n",
    "training_epochs=50\n",
    "batch_size=100\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "#COST로 Softmax사용!\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X:batch_xs, Y: batch_ys}\n",
    "        c,_=sess.run([cost,optimizer],feed_dict= feed_dict)\n",
    "        avg_cost += c/total_batch\n",
    "    \n",
    "    print('Epoch:','%04d'%(epoch+1),'cost=','{:.15f}'.format(avg_cost))\n",
    "print('Learning Finished!')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print('Accuracy:',sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network for MNIST (Accuracy : 94 % )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 142.06663675481598829719587229192256927\n",
      "Epoch: 0002 cost= 37.91883633180097490367188584059476852\n",
      "Epoch: 0003 cost= 23.89130161805585927936590451281517744\n",
      "Epoch: 0004 cost= 16.56827908212488864592160098254680634\n",
      "Epoch: 0005 cost= 12.14815147289472463398851687088608742\n",
      "Epoch: 0006 cost= 8.88016841530799894144365680404007435\n",
      "Epoch: 0007 cost= 6.80209502841536473027872489183209836\n",
      "Epoch: 0008 cost= 5.05860465671922288066753026214428246\n",
      "Epoch: 0009 cost= 3.75601373171531038863690810103435069\n",
      "Epoch: 0010 cost= 2.83145360136955881458220574131701142\n",
      "Epoch: 0011 cost= 2.09764883143404690102329368528444320\n",
      "Epoch: 0012 cost= 1.61253131141859351593836890970123932\n",
      "Epoch: 0013 cost= 1.28767592089746707451070051320130005\n",
      "Epoch: 0014 cost= 1.03426959628493531084814094356261194\n",
      "Epoch: 0015 cost= 0.72670185737591097563381481450051069\n",
      "Learning Finished!\n",
      "Accuracy: 0.9488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=100\n",
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32,[None,784])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "W1= tf.Variable(tf.random_normal([784,256]))\n",
    "b1=tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1) ##relu사용과 NN을 통한 성능 향상 \n",
    "\n",
    "W2= tf.Variable(tf.random_normal([256,256])) ##이 값은 아무거나 해도 상관없다\n",
    "b2=tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2) ##relu사용과 NN을 통한 성능 향상 \n",
    "\n",
    "W3= tf.Variable(tf.random_normal([256,10]))\n",
    "b3=tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "\n",
    "hypothesis = tf.matmul(L2,W3)+b3 ##relu사용과 NN을 통한 성능 향상 \n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X:batch_xs, Y: batch_ys}\n",
    "        c,_=sess.run([cost,optimizer],feed_dict= feed_dict)\n",
    "        avg_cost += c/total_batch\n",
    "    \n",
    "    print('Epoch:','%04d'%(epoch+1),'cost=','{:.35f}'.format(avg_cost))\n",
    "print('Learning Finished!')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print('Accuracy:',sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier, Deep NN , DropOut, AdamOptimizer for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-76b6eab6c870>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\hyunju\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\hyunju\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hyunju\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hyunju\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\hyunju\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-76b6eab6c870>:42: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost= 0.57266981813040696014383001966052689\n",
      "Epoch: 0002 cost= 0.21976852895184023628516456483339425\n",
      "Epoch: 0003 cost= 0.16631775085898950483631608676660107\n",
      "Epoch: 0004 cost= 0.13779173387045218213131647644331679\n",
      "Epoch: 0005 cost= 0.12413315096023408434788848353491630\n",
      "Epoch: 0006 cost= 0.10817651308734312076698103055605316\n",
      "Epoch: 0007 cost= 0.10232554614459249409108565487258602\n",
      "Epoch: 0008 cost= 0.09082234928045755661951687898181262\n",
      "Epoch: 0009 cost= 0.08519146709448914556173093615143443\n",
      "Epoch: 0010 cost= 0.08022327295081185805081247508496745\n",
      "Epoch: 0011 cost= 0.07782191066240720656033147406560602\n",
      "Epoch: 0012 cost= 0.07152812505987563018727826147369342\n",
      "Epoch: 0013 cost= 0.06721851317635316180876259295473574\n",
      "Epoch: 0014 cost= 0.06461122081022369068037392025871668\n",
      "Epoch: 0015 cost= 0.06324185772629620228002522708266042\n",
      "Epoch: 0016 cost= 0.06061778516000641825467454282261315\n",
      "Epoch: 0017 cost= 0.05715478612575676520268785907319398\n",
      "Epoch: 0018 cost= 0.05436791693241419670679093201215437\n",
      "Epoch: 0019 cost= 0.05329133415002032525542574603605317\n",
      "Epoch: 0020 cost= 0.05436365644447509193337708666149410\n",
      "Epoch: 0021 cost= 0.04868772241054104138457958583785512\n",
      "Epoch: 0022 cost= 0.05371327599468218860767620981278014\n",
      "Epoch: 0023 cost= 0.05062436300901359403914270274071896\n",
      "Epoch: 0024 cost= 0.04729091367218640762359882501186803\n",
      "Epoch: 0025 cost= 0.04615220808000726482323017307862756\n",
      "Epoch: 0026 cost= 0.04934578140075741858883162649362930\n",
      "Epoch: 0027 cost= 0.04497463587481019542391180721097044\n",
      "Epoch: 0028 cost= 0.04072315740907050951591372722759843\n",
      "Epoch: 0029 cost= 0.04448336758597920548297111054125708\n",
      "Epoch: 0030 cost= 0.04325408059542740907277291739774228\n",
      "Epoch: 0031 cost= 0.03927680852955253015812431272024696\n",
      "Epoch: 0032 cost= 0.04073110025286125202015341528749559\n",
      "Epoch: 0033 cost= 0.03970219597982419662551833994257322\n",
      "Epoch: 0034 cost= 0.03954793912665494765024121193164319\n",
      "Epoch: 0035 cost= 0.03679500266602687974515362157035270\n",
      "Epoch: 0036 cost= 0.03819136276745915131725084279423754\n",
      "Epoch: 0037 cost= 0.03943317927525969573965269887594332\n",
      "Epoch: 0038 cost= 0.03911015879923728649769643084255222\n",
      "Epoch: 0039 cost= 0.03645899992520840876553123166559089\n",
      "Epoch: 0040 cost= 0.03788553481477057588433510204595223\n",
      "Epoch: 0041 cost= 0.03393651295654390165346114827116253\n",
      "Epoch: 0042 cost= 0.03090157821549588748610837285468733\n",
      "Epoch: 0043 cost= 0.03791746288569733391859983839822235\n",
      "Epoch: 0044 cost= 0.03299981677677180785801525075839891\n",
      "Epoch: 0045 cost= 0.03562838724869917678139685790483782\n",
      "Epoch: 0046 cost= 0.03825219270588027520307861095716362\n",
      "Epoch: 0047 cost= 0.03380798567282248395793331496861356\n",
      "Epoch: 0048 cost= 0.03261242545019329280853170871523616\n",
      "Epoch: 0049 cost= 0.03207649956317090017554605196892226\n",
      "Epoch: 0050 cost= 0.03297621894611934417751442083499569\n",
      "Learning Finished!\n",
      "Accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "\n",
    "\n",
    "learning_rate=0.001\n",
    "training_epochs=50\n",
    "batch_size=100\n",
    "\n",
    "X= tf.placeholder(tf.float32,[None,784])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob =tf.placeholder(tf.float32)\n",
    "\n",
    "W1= tf.get_variable(\"W1\",shape=[784,256],       initializer=tf.contrib.layers.xavier_initializer()) ##Xavier Initializer를 통한 성능 향상\n",
    "b1=tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)  ##relu사용과 NN을 통한 성능 향상 \n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2= tf.get_variable(\"W2\",shape=[256,256],  initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3= tf.get_variable(\"W3\",shape=[256,256],  initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([256]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "W4= tf.get_variable(\"W4\",shape=[256,256],       initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([256]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3,W4)+b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob) #과적합 방지를 위해 반드시 dropout수행!!!\n",
    "\n",
    "W5= tf.get_variable(\"W5\",shape=[256,10],     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4,W5)+b5\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#Gradientdescent 대신 adamoptimizer 를 통한 성능 향상\n",
    "\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X:batch_xs, Y: batch_ys, keep_prob:0.7} #training에선 dropout수치 0.7\n",
    "        c,_=sess.run([cost,optimizer],feed_dict= feed_dict)\n",
    "        avg_cost += c/total_batch\n",
    "    \n",
    "    print('Epoch:','%04d'%(epoch+1),'cost=','{:.35f}'.format(avg_cost))\n",
    "print('Learning Finished!')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print('Accuracy:',sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels, keep_prob:1})) \n",
    "                            #training에선 dropout수치 1.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
