{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Autograd: 자동 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd Package\n",
    "\n",
    "Autograd 패키지는 tensor의 모든 연산에 자동 미분을 제공합니다. 이는 define-by-run의 프레임워크로 코드를 어떻게 작성하느냐에 따라 역전파가 정의된다는 뜻입니다. 역전파는 학습과정의 매 단계마다 달라집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2,2,requires_grad=True) #tensor를 생성하고 requires_grad=True로 연산을 기록합니다.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "y = x+2 #gradient function이 자동으로 포함됩니다.\n",
    "print(y)\n",
    "\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward1>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(out) # out = 3(x+2)*2\n",
    "out.backward()\n",
    "\n",
    "print(x)\n",
    "print(x.grad) # d(out)/dx 를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.3251,  3.3126,  1.5409], grad_fn=<MulBackward0>) tensor(7.3045)\n",
      "tensor([-12.6503,   6.6253,   3.0818], grad_fn=<MulBackward0>) tensor(14.6090)\n",
      "tensor([-25.3006,  13.2505,   6.1637], grad_fn=<MulBackward0>) tensor(29.2179)\n",
      "tensor([-50.6012,  26.5010,  12.3274], grad_fn=<MulBackward0>) tensor(58.4358)\n",
      "tensor([-101.2023,   53.0021,   24.6547], grad_fn=<MulBackward0>) tensor(116.8717)\n",
      "tensor([-202.4046,  106.0042,   49.3094], grad_fn=<MulBackward0>) tensor(233.7433)\n",
      "tensor([-404.8093,  212.0083,   98.6189], grad_fn=<MulBackward0>) tensor(467.4867)\n",
      "tensor([-809.6186,  424.0167,  197.2377], grad_fn=<MulBackward0>) tensor(934.9733)\n",
      "tensor([-1619.2372,   848.0333,   394.4754], grad_fn=<MulBackward0>) tensor(1869.9467)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "\n",
    "y=x*2\n",
    "\n",
    "while y.data.norm() < 1000:\n",
    "    \n",
    "    #data.norm()은 점들 사이의 유클리디안 거리를 나타냅니다\n",
    "    #torch.sqrt(torch.sum(torch.pow(y, 2)))\n",
    "    \n",
    "    y = y*2\n",
    "    \n",
    "    print(y,y.data.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1619.2372,   848.0333,   394.4754], grad_fn=<MulBackward0>)\n",
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1,1.0,0.0001],dtype=torch.float)\n",
    "print(y)\n",
    "y.backward(gradients)\n",
    "print(x.grad) # d(y)/d(x) 를 출력합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1주차 과제 ))   MNIST 정확도 92% 이상 만들어보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "batch_size=32\n",
    "learning_rate=0.01\n",
    "num_epochs=5\n",
    "\n",
    "train_dataset=datasets.MNIST(root='./data',train =True, \n",
    "                             transform = transforms.ToTensor(), download=True)\n",
    "test_dataset=datasets.MNIST(root='./data',train =False, \n",
    "                             transform = transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle = True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.layer1=nn.Linear(28*28,100)\n",
    "        self.layer2=nn.Linear(100,200)\n",
    "        self.layer3=nn.Linear(200,num_classes)\n",
    "        \n",
    "        def forward(self,x):\n",
    "            out=x.view(x.size(0),-1)\n",
    "            out=self.layer1(out)\n",
    "            out=self.layer2(out)\n",
    "            out=self.layer3(out)\n",
    "            return out\n",
    "#MODEL\n",
    "model=NeuralNetwork()\n",
    "#LOSS\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#OPTIMIZER\n",
    "optimiizer = optim.SGD(model.parameters(),lr=learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-fb4e2086b479>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-fb4e2086b479>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#학습시켜주기\n",
    "for epoch in range(num_epochs):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
