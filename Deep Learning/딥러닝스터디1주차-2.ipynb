{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Autograd: 자동 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd Package\n",
    "\n",
    "Autograd 패키지는 tensor의 모든 연산에 자동 미분을 제공합니다. 이는 define-by-run의 프레임워크로 코드를 어떻게 작성하느냐에 따라 역전파가 정의된다는 뜻입니다. 역전파는 학습과정의 매 단계마다 달라집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2,2,requires_grad=True) #tensor를 생성하고 requires_grad=True로 연산을 기록합니다.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "y = x+2 #gradient function이 자동으로 포함됩니다.\n",
    "print(y)\n",
    "\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward1>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(out) # out = 3(x+2)*2\n",
    "out.backward()\n",
    "\n",
    "print(x)\n",
    "print(x.grad) # d(out)/dx 를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6321, -2.2635,  0.5391], grad_fn=<MulBackward0>) tensor(2.8422)\n",
      "tensor([ 3.2643, -4.5270,  1.0782], grad_fn=<MulBackward0>) tensor(5.6844)\n",
      "tensor([ 6.5285, -9.0541,  2.1565], grad_fn=<MulBackward0>) tensor(11.3688)\n",
      "tensor([ 13.0571, -18.1082,   4.3130], grad_fn=<MulBackward0>) tensor(22.7376)\n",
      "tensor([ 26.1142, -36.2164,   8.6259], grad_fn=<MulBackward0>) tensor(45.4751)\n",
      "tensor([ 52.2284, -72.4328,  17.2519], grad_fn=<MulBackward0>) tensor(90.9502)\n",
      "tensor([ 104.4568, -144.8656,   34.5038], grad_fn=<MulBackward0>) tensor(181.9004)\n",
      "tensor([ 208.9135, -289.7312,   69.0075], grad_fn=<MulBackward0>) tensor(363.8008)\n",
      "tensor([ 417.8271, -579.4623,  138.0151], grad_fn=<MulBackward0>) tensor(727.6017)\n",
      "tensor([  835.6542, -1158.9247,   276.0302], grad_fn=<MulBackward0>) tensor(1455.2034)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "\n",
    "y=x*2\n",
    "\n",
    "while y.data.norm() < 1000:\n",
    "    \n",
    "    #data.norm()은 점들 사이의 유클리디안 거리를 나타냅니다\n",
    "    #torch.sqrt(torch.sum(torch.pow(y, 2)))\n",
    "    \n",
    "    y = y*2\n",
    "    \n",
    "    print(y,y.data.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  835.6542, -1158.9247,   276.0302], grad_fn=<MulBackward0>)\n",
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1,1.0,0.0001],dtype=torch.float)\n",
    "print(y)\n",
    "y.backward(gradients)\n",
    "print(x.grad) # d(y)/d(x) 를 출력합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1주차 과제 ))   MNIST 정확도 92% 이상 만들어보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "batch_size=32\n",
    "learning_rate=0.06\n",
    "num_epochs=5\n",
    "\n",
    "train_dataset=datasets.MNIST(root='./data',train =True, \n",
    "                             transform = transforms.ToTensor(), download=True)\n",
    "test_dataset=datasets.MNIST(root='./data',train =False, \n",
    "                             transform = transforms.ToTensor())\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle = True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.layer1=nn.Linear(28*28,100)\n",
    "        self.layer2=nn.Linear(100,200)\n",
    "        self.layer3=nn.Linear(200,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=x.view(x.size(0),-1)\n",
    "        out=self.layer1(out)\n",
    "        out=self.layer2(out)\n",
    "        out=self.layer3(out)\n",
    "        return out\n",
    "#MODEL\n",
    "model=NeuralNetwork()\n",
    "#LOSS\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#OPTIMIZER\n",
    "optimizer = optim.SGD(model.parameters(),lr=learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[100/1875], Loss :0.9944\n",
      "Epoch [1/5], Step[200/1875], Loss :0.4209\n",
      "Epoch [1/5], Step[300/1875], Loss :0.1991\n",
      "Epoch [1/5], Step[400/1875], Loss :0.4513\n",
      "Epoch [1/5], Step[500/1875], Loss :0.3930\n",
      "Epoch [1/5], Step[600/1875], Loss :0.3410\n",
      "Epoch [1/5], Step[700/1875], Loss :0.9456\n",
      "Epoch [1/5], Step[800/1875], Loss :0.1546\n",
      "Epoch [1/5], Step[900/1875], Loss :0.3402\n",
      "Epoch [1/5], Step[1000/1875], Loss :0.1389\n",
      "Epoch [1/5], Step[1100/1875], Loss :0.2339\n",
      "Epoch [1/5], Step[1200/1875], Loss :0.0811\n",
      "Epoch [1/5], Step[1300/1875], Loss :0.2637\n",
      "Epoch [1/5], Step[1400/1875], Loss :0.0834\n",
      "Epoch [1/5], Step[1500/1875], Loss :0.5032\n",
      "Epoch [1/5], Step[1600/1875], Loss :0.1895\n",
      "Epoch [1/5], Step[1700/1875], Loss :0.3416\n",
      "Epoch [1/5], Step[1800/1875], Loss :0.3083\n",
      "Epoch [2/5], Step[100/1875], Loss :0.3484\n",
      "Epoch [2/5], Step[200/1875], Loss :0.2994\n",
      "Epoch [2/5], Step[300/1875], Loss :0.3813\n",
      "Epoch [2/5], Step[400/1875], Loss :0.1189\n",
      "Epoch [2/5], Step[500/1875], Loss :0.3531\n",
      "Epoch [2/5], Step[600/1875], Loss :0.1538\n",
      "Epoch [2/5], Step[700/1875], Loss :0.1491\n",
      "Epoch [2/5], Step[800/1875], Loss :0.2708\n",
      "Epoch [2/5], Step[900/1875], Loss :0.0884\n",
      "Epoch [2/5], Step[1000/1875], Loss :0.5353\n",
      "Epoch [2/5], Step[1100/1875], Loss :0.1086\n",
      "Epoch [2/5], Step[1200/1875], Loss :0.4466\n",
      "Epoch [2/5], Step[1300/1875], Loss :0.2359\n",
      "Epoch [2/5], Step[1400/1875], Loss :0.1588\n",
      "Epoch [2/5], Step[1500/1875], Loss :0.2813\n",
      "Epoch [2/5], Step[1600/1875], Loss :0.3042\n",
      "Epoch [2/5], Step[1700/1875], Loss :0.2847\n",
      "Epoch [2/5], Step[1800/1875], Loss :0.5424\n",
      "Epoch [3/5], Step[100/1875], Loss :0.4672\n",
      "Epoch [3/5], Step[200/1875], Loss :0.1968\n",
      "Epoch [3/5], Step[300/1875], Loss :0.4913\n",
      "Epoch [3/5], Step[400/1875], Loss :0.3846\n",
      "Epoch [3/5], Step[500/1875], Loss :0.1731\n",
      "Epoch [3/5], Step[600/1875], Loss :0.1800\n",
      "Epoch [3/5], Step[700/1875], Loss :0.3894\n",
      "Epoch [3/5], Step[800/1875], Loss :0.1726\n",
      "Epoch [3/5], Step[900/1875], Loss :0.3252\n",
      "Epoch [3/5], Step[1000/1875], Loss :0.2411\n",
      "Epoch [3/5], Step[1100/1875], Loss :0.1575\n",
      "Epoch [3/5], Step[1200/1875], Loss :0.7024\n",
      "Epoch [3/5], Step[1300/1875], Loss :0.2915\n",
      "Epoch [3/5], Step[1400/1875], Loss :0.2951\n",
      "Epoch [3/5], Step[1500/1875], Loss :0.2564\n",
      "Epoch [3/5], Step[1600/1875], Loss :0.1253\n",
      "Epoch [3/5], Step[1700/1875], Loss :0.2034\n",
      "Epoch [3/5], Step[1800/1875], Loss :0.1538\n",
      "Epoch [4/5], Step[100/1875], Loss :0.2231\n",
      "Epoch [4/5], Step[200/1875], Loss :0.2258\n",
      "Epoch [4/5], Step[300/1875], Loss :0.1168\n",
      "Epoch [4/5], Step[400/1875], Loss :0.6578\n",
      "Epoch [4/5], Step[500/1875], Loss :0.1997\n",
      "Epoch [4/5], Step[600/1875], Loss :0.4185\n",
      "Epoch [4/5], Step[700/1875], Loss :0.2245\n",
      "Epoch [4/5], Step[800/1875], Loss :0.1050\n",
      "Epoch [4/5], Step[900/1875], Loss :0.2071\n",
      "Epoch [4/5], Step[1000/1875], Loss :0.3787\n",
      "Epoch [4/5], Step[1100/1875], Loss :0.1012\n",
      "Epoch [4/5], Step[1200/1875], Loss :0.4880\n",
      "Epoch [4/5], Step[1300/1875], Loss :0.1971\n",
      "Epoch [4/5], Step[1400/1875], Loss :0.5539\n",
      "Epoch [4/5], Step[1500/1875], Loss :0.1410\n",
      "Epoch [4/5], Step[1600/1875], Loss :0.2154\n",
      "Epoch [4/5], Step[1700/1875], Loss :0.1334\n",
      "Epoch [4/5], Step[1800/1875], Loss :0.5344\n",
      "Epoch [5/5], Step[100/1875], Loss :0.2034\n",
      "Epoch [5/5], Step[200/1875], Loss :0.3921\n",
      "Epoch [5/5], Step[300/1875], Loss :0.3920\n",
      "Epoch [5/5], Step[400/1875], Loss :0.3425\n",
      "Epoch [5/5], Step[500/1875], Loss :0.3971\n",
      "Epoch [5/5], Step[600/1875], Loss :0.4636\n",
      "Epoch [5/5], Step[700/1875], Loss :0.5386\n",
      "Epoch [5/5], Step[800/1875], Loss :0.4314\n",
      "Epoch [5/5], Step[900/1875], Loss :0.3878\n",
      "Epoch [5/5], Step[1000/1875], Loss :0.3054\n",
      "Epoch [5/5], Step[1100/1875], Loss :0.4190\n",
      "Epoch [5/5], Step[1200/1875], Loss :0.1984\n",
      "Epoch [5/5], Step[1300/1875], Loss :0.2104\n",
      "Epoch [5/5], Step[1400/1875], Loss :0.2534\n",
      "Epoch [5/5], Step[1500/1875], Loss :0.4483\n",
      "Epoch [5/5], Step[1600/1875], Loss :0.3911\n",
      "Epoch [5/5], Step[1700/1875], Loss :0.3406\n",
      "Epoch [5/5], Step[1800/1875], Loss :0.3765\n"
     ]
    }
   ],
   "source": [
    "#학습시켜주기\n",
    "for epoch in range(num_epochs):\n",
    "    for i , (img,label) in enumerate(train_loader,1):\n",
    "        img,label=Variable(img),Variable(label)\n",
    "        out=model(img)\n",
    "        loss=criterion(out,label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i+1)%100 == 0:\n",
    "            print('Epoch [{}/{}], Step[{}/{}], Loss :{:.4f}'.format(epoch+1,num_epochs,i+1,len(train_loader),loss.item()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images : 92.35 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0 \n",
    "total = 0\n",
    "for img, label in test_loader:\n",
    "    out=model(img)\n",
    "    _, predicted = torch.max(out.data,1)\n",
    "    total+=label.size(0)\n",
    "    correct +=(predicted==label).sum().item()\n",
    "    \n",
    "print('Test Accuracy of the model on the 10000 test images : {} %'.format(100*correct/total))\n",
    "torch.save(model.state_dict(),'model_nn.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
